{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MnemosyneRAG: A Lightweight RAG Implementation with Advanced Caching\n",
    "\n",
    "A fast and efficient RAG system built with ChromaDB, featuring multi-level caching and asynchronous operations.\n",
    "\n",
    "Features:\n",
    "- Two-level caching (memory for embeddings, ChromaDB for responses)\n",
    "- Asynchronous background caching\n",
    "- Thread-safe operations\n",
    "- Response times < 50ms on cache hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "First, let's import all required libraries and set up our environment.\n",
    "\n",
    "````\n",
    "pip install openai chromadb pydantic python-dotenv tenacity\n",
    "````\n",
    "\n",
    "Create a .env file with the content from below. Make sure you fulfill the required information\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=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\n",
    "CLIENT_ID=1\n",
    "CLIENT_SECRET=2\n",
    "ANONYMIZED_TELEMETRY=False\n",
    "```\n",
    "\n",
    "You will need to create a chroma_db vector db with two collections, kb_knowledge to store the KB data and query_cache to use it as temporal caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "import threading\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from functools import lru_cache\n",
    "import json\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import asyncio\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Define our configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    CHROMA_PATH: str = \"./chroma_db\"\n",
    "    COLLECTION_NAME: str = \"kb_knowledge\"\n",
    "    CACHE_COLLECTION_NAME: str = \"query_cache\"\n",
    "    SIMILARITY_THRESHOLD: float = 0.9\n",
    "    CACHE_SIZE: int = 1000\n",
    "    DEFAULT_N_RESULTS: int = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Services\n",
    "Set up our OpenAI and ChromaDB clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_services():\n",
    "    # Initialize OpenAI client\n",
    "    openai_client = OpenAI()\n",
    "    \n",
    "    # Initialize ChromaDB\n",
    "    chroma_client = chromadb.PersistentClient(path=Config.CHROMA_PATH)\n",
    "    kb_collection = chroma_client.get_collection(name=Config.COLLECTION_NAME)\n",
    "    cache_collection = chroma_client.get_or_create_collection(name=Config.CACHE_COLLECTION_NAME)\n",
    "    \n",
    "    return openai_client, kb_collection, cache_collection\n",
    "\n",
    "# Initialize services\n",
    "openai_client, kb_collection, cache_collection = init_services()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Models\n",
    "Define our Pydantic models for type safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserQuery(BaseModel):\n",
    "    question: str = Field(..., min_length=1, max_length=1000)\n",
    "    \n",
    "class CacheResponse(BaseModel):\n",
    "    response: str\n",
    "    documents: List[List[str]]\n",
    "    metadata: List[List[Dict]]\n",
    "    links: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Functions\n",
    "Functions for handling embeddings with caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=Config.CACHE_SIZE)\n",
    "def get_cached_embedding(text: str) -> tuple:\n",
    "    \"\"\"Cache embeddings for frequently asked questions\"\"\"\n",
    "    embedding = get_embedding(text)\n",
    "    return tuple(embedding)\n",
    "\n",
    "@retry(\n",
    "    wait=wait_random_exponential(min=1, max=20),\n",
    "    stop=stop_after_attempt(6)\n",
    ")\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get embeddings with retry logic\"\"\"\n",
    "    try:\n",
    "        return openai_client.embeddings.create(\n",
    "            input=[text],\n",
    "            model=\"text-embedding-3-small\"\n",
    "        ).data[0].embedding\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Embedding generation failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingCache:\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "        self._lock = threading.Lock()\n",
    "\n",
    "    def get(self, key: str) -> Optional[List[float]]:\n",
    "        with self._lock:\n",
    "            return self.cache.get(key)\n",
    "\n",
    "    def set(self, key: str, value: List[float]):\n",
    "        with self._lock:\n",
    "            if len(self.cache) >= self.max_size:\n",
    "                self.cache.pop(next(iter(self.cache)))\n",
    "            self.cache[key] = value\n",
    "\n",
    "# Initialize global cache and thread pool\n",
    "embedding_cache = EmbeddingCache()\n",
    "thread_pool = ThreadPoolExecutor(max_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_response_background(query: str, response: CacheResponse):\n",
    "    \"\"\"Background task for caching responses\"\"\"\n",
    "    try:\n",
    "        cache_key = hashlib.md5(query.encode()).hexdigest()\n",
    "        embedding = embedding_cache.get(cache_key)\n",
    "        \n",
    "        if not embedding:\n",
    "            embedding = get_embedding(query)\n",
    "            embedding_cache.set(cache_key, embedding)\n",
    "\n",
    "        cache_id = str(uuid.uuid4())\n",
    "        \n",
    "        cache_collection.add(\n",
    "            ids=[cache_id],\n",
    "            documents=[query],\n",
    "            embeddings=[embedding],\n",
    "            metadatas=[{\n",
    "                \"response\": response.response,\n",
    "                \"documents\": json.dumps(response.documents),\n",
    "                \"metadata\": json.dumps(response.metadata),\n",
    "                \"links\": json.dumps(response.links)\n",
    "            }]\n",
    "        )\n",
    "        print(f\"Successfully cached response with ID: {cache_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Background caching failed: {str(e)}\")\n",
    "\n",
    "async def check_cache(query: str) -> Optional[CacheResponse]:\n",
    "    \"\"\"Check cache for similar queries\"\"\"\n",
    "    try:\n",
    "        cache_key = hashlib.md5(query.encode()).hexdigest()\n",
    "        embedding = embedding_cache.get(cache_key)\n",
    "        \n",
    "        if not embedding:\n",
    "            embedding = get_embedding(query)\n",
    "            embedding_cache.set(cache_key, embedding)\n",
    "\n",
    "        results = cache_collection.query(\n",
    "            query_embeddings=[embedding],\n",
    "            n_results=1\n",
    "        )\n",
    "        \n",
    "        if not results[\"ids\"] or not results[\"metadatas\"]:\n",
    "            return None\n",
    "\n",
    "        if results[\"distances\"][0][0] > 0.1:\n",
    "            return None\n",
    "\n",
    "        metadata = results[\"metadatas\"][0][0]\n",
    "        return CacheResponse(\n",
    "            response=metadata[\"response\"],\n",
    "            documents=json.loads(metadata[\"documents\"]),\n",
    "            metadata=json.loads(metadata[\"metadata\"]),\n",
    "            links=json.loads(metadata[\"links\"])\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Cache check failed: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_query(query: str) -> CacheResponse:\n",
    "    \"\"\"Process a query through the RAG system\"\"\"\n",
    "    try:\n",
    "        # Check cache first\n",
    "        cached_response = await check_cache(query)\n",
    "        if cached_response:\n",
    "            print(\"Cache hit! Returning cached response\")\n",
    "            return cached_response\n",
    "\n",
    "        print(\"Cache miss, generating new response\")\n",
    "        \n",
    "        # Query knowledge base\n",
    "        kb_results = kb_collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=1\n",
    "        )\n",
    "        \n",
    "        if not kb_results[\"documents\"] or not kb_results[\"documents\"][0]:\n",
    "            raise ValueError(\"No relevant documents found\")\n",
    "\n",
    "        # Generate context and prepare messages\n",
    "        context = \"\\n\\n\".join(kb_results[\"documents\"][0])\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. If a user's request is unclear, request clarification.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Context: {context}\\n\\nQuestion: {query}\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Generate LLM response\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        result = CacheResponse(\n",
    "            response=response.choices[0].message.content,\n",
    "            documents=kb_results[\"documents\"],\n",
    "            metadata=kb_results[\"metadatas\"],\n",
    "            links=[f\"https://example.link/{meta['number']}\" for meta in kb_results[\"metadatas\"][0]]\n",
    "        )\n",
    "        \n",
    "        # Cache in background\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_in_executor(\n",
    "            thread_pool,\n",
    "            cache_response_background,\n",
    "            query,\n",
    "            result\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to process question: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "Test the RAG system with a sample query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_system():\n",
    "    query = \"What is machine learning?\"\n",
    "    try:\n",
    "        result = await process_query(query)\n",
    "        print(\"Response:\", result.response)\n",
    "        print(\"\\nDocuments:\", result.documents)\n",
    "        print(\"\\nLinks:\", result.links)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Run the test\n",
    "await test_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Management\n",
    "Functions to manage the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def clear_cache(cache_id: Optional[str] = None):\n",
    "    \"\"\"Clear the cache either completely or for a specific ID\"\"\"\n",
    "    try:\n",
    "        if cache_id:\n",
    "            cache_collection.delete(ids=[cache_id])\n",
    "            print(f\"Cache cleared for ID: {cache_id}\")\n",
    "            return\n",
    "            \n",
    "        all_ids = cache_collection.get()[\"ids\"]\n",
    "        if all_ids:\n",
    "            cache_collection.delete(ids=all_ids)\n",
    "            print(\"Complete cache cleared successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Cache clearing failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Example: Multiple Queries\n",
    "Test the system with multiple queries to demonstrate caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_multiple_queries():\n",
    "    queries = [\n",
    "        \"What is machine learning?\",\n",
    "        \"Explain artificial intelligence\",\n",
    "        \"What is machine learning?\"  # Repeated query to test cache\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nProcessing query: {query}\")\n",
    "        try:\n",
    "            result = await process_query(query)\n",
    "            print(\"Response:\", result.response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "        \n",
    "# Run multiple queries test\n",
    "await test_multiple_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "Measure response times with and without cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "async def measure_performance():\n",
    "    query = \"What is machine learning?\"\n",
    "    \n",
    "    # First query (no cache)\n",
    "    start_time = time.time()\n",
    "    result1 = await process_query(query)\n",
    "    first_query_time = time.time() - start_time\n",
    "    \n",
    "    # Second query (should hit cache)\n",
    "    start_time = time.time()\n",
    "    result2 = await process_query(query)\n",
    "    second_query_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"First query (no cache): {first_query_time:.2f} seconds\")\n",
    "    print(f\"Second query (cache hit): {second_query_time:.2f} seconds\")\n",
    "\n",
    "# Run performance test\n",
    "await measure_performance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
